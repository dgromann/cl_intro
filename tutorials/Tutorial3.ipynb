{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/cl_intro/blob/main/tutorials/Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GszGq3W2gEWm"
      },
      "source": [
        "# Tutorial 3: Introduction to Computational Linguistics\n",
        "\n",
        "This is the second tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2023. Hands-on exercises are marked with üëã ‚öí and questions are marked with ‚ùì. Remember to first **store this notebook** in your Drive or GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnC6HAtqgHGK"
      },
      "source": [
        "---\n",
        "\n",
        "## **Lesson 3: Word Embeddings**\n",
        "\n",
        "A vector representation of words trained with a neural network is called a word embedding. The most popular method for training embeddings is called word2vec, which is an unsupervised method of training embeddings from large natural language corpora.\n",
        "\n",
        "\n",
        "`word2vec` literature:\n",
        "  - Mikolov, T.,  Chen, K., Corrado, G., & Dean, J. (2013). [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781). Corr abs/1301.3781.\n",
        "  - Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). [Distributed representations of words and phrases and their compositionally](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). *Advances in neural information processing systems*. 2013.\n",
        "\n",
        "\n",
        "- Other variants of embeddings training:\n",
        "  - `fasttext` from Facebook\n",
        "  - `GloVe` from Stanford NLP Group\n",
        "- There are many ways to train work embeddings.\n",
        "  - `gensim`: Simplest and straightforward implementation of `word2vec`.\n",
        "  - Training based on deep learning packages (e.g., `keras`, `tensorflow`)\n",
        "  - `spacy` (It comes with the pre-trained embeddings models, using GloVe.)\n",
        "- See Sarkar (2019), Chapter 4, for more comprehensive reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uPkYI6PlNgp"
      },
      "source": [
        "If we assume our training corpus contains 10,000 unique words, our input vector to the model to train embeddings with skipgram has 10,000 dimensions, one for each word in the vocabulary.\n",
        "\n",
        "In the input vector all dimensions are zero but one, which indicates the word in the vocabulary, e.g. \"ant\" in the example below. This is why these vectors are called one-hot encodings.\n",
        "\n",
        "\n",
        "Source of the following image: [Chris McCormik Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDlljZFIliwD"
      },
      "source": [
        "![architecture](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfcYOrYDmpfv"
      },
      "source": [
        "When multiplying a matrix on the hidden layer with the one-hot encoding, we obtain one row of the matrix. So the matrix serves as a lookup table for embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz090_Qen13m"
      },
      "source": [
        "![matrix_mult](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysBasqzNn5yP"
      },
      "source": [
        "The output layer is a softmax regression classifier, which changes the rates of the weights into a range between 0 and 1, where the sum of all dimensions add up to 1. The final output layer indicates the probabilty of context words for the input, e.g. how high is the probability of \"ability\" occuring near to \"ant\".\n",
        "\n",
        "Since learning embeddings based on all context words anywhere near a word in a text is inefficient, a window size is selected to limit the number of context words that are considered during training:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwnSyAtChC4z"
      },
      "source": [
        "![word2vec_skipgrams](https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRS9y_W1pv_C"
      },
      "source": [
        "### Using Pre-Trained Embeddings\n",
        "\n",
        "The code below exemplifies how to load a trained embedding model in the gensim library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K0JS2yIf9ey"
      },
      "outputs": [],
      "source": [
        "# Let's first load a small subset of word2vec embeddings that have been trained on a\n",
        "# large corpus of news documents\n",
        "!wget https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/word2vec_embeddings.bin\n",
        "!wget https://raw.githubusercontent.com/dgromann/cl_intro/master/tutorials/Tutorial3.txt\n",
        "!pip3 install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT3equb6aKNy"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Let's load the model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec_embeddings.bin.3\", binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the length fo the whole vocabulary\n",
        "print(\"Length of the vocabulary\",len(model.key_to_index))\n",
        "\n",
        "# Print the embedding of a specific word\n",
        "print(\"Embedding for the word good: \", model[\"good\"])"
      ],
      "metadata": {
        "id": "D_Rm7ukcl0Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí How many dimensions (numbers) does each vector in this trained embedding model have? Try to find this out with code, not by counting."
      ],
      "metadata": {
        "id": "8Nb8E4AYl6SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code goes here"
      ],
      "metadata": {
        "id": "zJdLUqzamYIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use embeddings to evaluate how similar two words are.\n",
        "\n",
        "üëã ‚öí How can we get the first most similiar word of good from the list of the top 5 most similar words?"
      ],
      "metadata": {
        "id": "z0wPsvgHl3wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar('good'))\n",
        "\n",
        "# Get the top 5 most similar words of \"good\"\n",
        "most_similar = model.most_similar(\"good\", topn=5)\n",
        "print(most_similar)\n",
        "\n",
        "# Your code to get the first word from that list of top 5\n"
      ],
      "metadata": {
        "id": "hMaZSp_Wl34h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use these embeddings to obtain similar words in other pairs with the analogy task a is to b as c is to d, e.g. *man is to woman as king is to ?*"
      ],
      "metadata": {
        "id": "y4jr1MyxnaKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether our embeddings are good at the analogy task\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yfGt0BDl4AT",
        "outputId": "9401836b-137d-4e45-eb09-4e5a1f40a6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí How can we get the first most similiar word of good from the list of the top 5 most similar words?"
      ],
      "metadata": {
        "id": "rGaa-MRunjUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a, b, c):\n",
        "  #Your code goes here\n",
        "\n",
        "\n",
        "print(analogy(\"France\", \"Paris\", \"Austria\"))\n",
        "print(analogy(\"good\", \"best\", \"bad\"))"
      ],
      "metadata": {
        "id": "FINqLoLbnlaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use matplotlib to visualize the proximity of words in vector space."
      ],
      "metadata": {
        "id": "cntAtMnkugSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pca_scatterplot(model, words):\n",
        "\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "metadata": {
        "id": "k5VAQte-pzGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(model,\n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'water',\n",
        "                         'hamburger', 'pizza',  'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'lizard',\n",
        "                         'France', 'Germany', 'Hungary',\n",
        "                         'school', 'college', 'university', 'institute'])"
      ],
      "metadata": {
        "id": "fZGXtTzGqG3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Write a function that runs through the text file analogy.txt and test your analogy function on each line apart from the header."
      ],
      "metadata": {
        "id": "PaSB9trppFER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analogy = open(\"Tutorial3.txt\", \"r\")\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "55O6Hvr1pJ4T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpkr3mw94uBea3OeSqSQ0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}