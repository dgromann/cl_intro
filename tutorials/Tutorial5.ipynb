{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwcW8CNbd2m0dbtNLQsGG5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c0918538de84619a286443876e30f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b31bc58ffec846ad8976e29f89fc8aec",
              "IPY_MODEL_39a69a18d8bb405fa488dc55bd831d12",
              "IPY_MODEL_a93f320ae32243458daefc3716fd268b"
            ],
            "layout": "IPY_MODEL_69310e2d8e93476ea2272fcf26fde000"
          }
        },
        "b31bc58ffec846ad8976e29f89fc8aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caca1eac3fcc4278b1804c40d0a2db97",
            "placeholder": "​",
            "style": "IPY_MODEL_c54720d3544047e696bc90c922fca546",
            "value": "config.json: 100%"
          }
        },
        "39a69a18d8bb405fa488dc55bd831d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a470fb9e9442ff81d22d388fd09fab",
            "max": 615,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c880f0691fd440aa95fb729cbc11d96e",
            "value": 615
          }
        },
        "a93f320ae32243458daefc3716fd268b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dda293ea7254ecea1847b48cceda679",
            "placeholder": "​",
            "style": "IPY_MODEL_eabcd881271c4f298713f863bcf6a3b4",
            "value": " 615/615 [00:00&lt;00:00, 33.3kB/s]"
          }
        },
        "69310e2d8e93476ea2272fcf26fde000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caca1eac3fcc4278b1804c40d0a2db97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c54720d3544047e696bc90c922fca546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45a470fb9e9442ff81d22d388fd09fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c880f0691fd440aa95fb729cbc11d96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dda293ea7254ecea1847b48cceda679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eabcd881271c4f298713f863bcf6a3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "777bc122c43f4ddf81089792f455ae8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe2b8a2d9261465f8bf14afd60171f7a",
              "IPY_MODEL_2154ba7754094d4c80004c04be847f80",
              "IPY_MODEL_f045588006d94b57bb278b8dacaa12f7"
            ],
            "layout": "IPY_MODEL_a0497345e21c409f84c070c16da62a22"
          }
        },
        "fe2b8a2d9261465f8bf14afd60171f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee22471aa35446e8dbcc107316c8600",
            "placeholder": "​",
            "style": "IPY_MODEL_2f5ce43840934c8cb8a6b69534ae50fd",
            "value": "model.safetensors: 100%"
          }
        },
        "2154ba7754094d4c80004c04be847f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d904479beefa440db6aa8beb8cf30256",
            "max": 1115567652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e982db3b7164f0a8cdf9af2c7c1dd60",
            "value": 1115567652
          }
        },
        "f045588006d94b57bb278b8dacaa12f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98bdcb6ef1cd4b2abd9117a85b016729",
            "placeholder": "​",
            "style": "IPY_MODEL_271d2901952841f1a46f2509f5e6160a",
            "value": " 1.12G/1.12G [00:14&lt;00:00, 150MB/s]"
          }
        },
        "a0497345e21c409f84c070c16da62a22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ee22471aa35446e8dbcc107316c8600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5ce43840934c8cb8a6b69534ae50fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d904479beefa440db6aa8beb8cf30256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e982db3b7164f0a8cdf9af2c7c1dd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98bdcb6ef1cd4b2abd9117a85b016729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "271d2901952841f1a46f2509f5e6160a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/cl_intro/blob/main/tutorials/Tutorial5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 5: Introduction to Computational Linguistics\n",
        "\n",
        "This is the second tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2023. Hands-on exercises are marked with 👋 ⚒ and questions are marked with ❓. Remember to first **store this notebook** in your Drive or GitHub."
      ],
      "metadata": {
        "id": "gZ9Hk2B4Sq3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson 4: Hugging Face Tutorial**\n",
        "\n",
        "Hugging Face is a platform that provides access to models, datasets, and metrics. It has implementations in PyTorch, TensorFlow , and [Flax](https://github.com/google/flax).\n",
        "\n",
        "\n",
        "In this tutorial, we will first focus on tokenizers and models. Afterwards, we will look into fine-tuning models. For further reading and to help your project, please check the [Hugging Face Documentation and Tutorials](https://huggingface.co/docs/transformers/index).\n",
        "\n",
        "As a first step, we need to install the libraries `transformers`to load any transformer models and the library `datasets`to have access to all the Hugging Face datasets."
      ],
      "metadata": {
        "id": "Mg3rQ5egSsYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chrmoR_USnc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac5260a-d956-46c5-fbe7-e04e8c9544b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
            "Collecting bertviz\n",
            "  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from bertviz) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bertviz) (4.66.1)\n",
            "Collecting boto3 (from bertviz)\n",
            "  Downloading boto3-1.34.14-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bertviz) (2.31.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from bertviz) (2023.6.3)\n",
            "Collecting sentencepiece (from bertviz)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (2.1.0)\n",
            "Collecting botocore<1.35.0,>=1.34.14 (from boto3->bertviz)\n",
            "  Downloading botocore-1.34.14-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->bertviz)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->bertviz)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.14->boto3->bertviz) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->bertviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.14->boto3->bertviz) (1.16.0)\n",
            "Installing collected packages: sentencepiece, jmespath, botocore, s3transfer, boto3, bertviz\n",
            "Successfully installed bertviz-1.4.0 boto3-1.34.14 botocore-1.34.14 jmespath-1.0.1 s3transfer-0.10.0 sentencepiece-0.1.99\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.25.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install bertviz transformers\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using Hugging Face, there are two very important objects to be initialized, a **tokenizer** and a **model**.\n",
        "\n",
        "\n",
        "\n",
        "*   **Tokenizer**: converts strings or text to lists of vocabularies required by the model\n",
        "*   **Model**: takes the tokenized datasets, i.e., the vocabulary ids, and produces a prediction\n",
        "\n",
        "We will start looking into the tokenizer first."
      ],
      "metadata": {
        "id": "R1fljaUSVMSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "\n",
        "Pre-trained language models are implemented with a tokenizer that processes their input. You can either access teoknizers with the class specific to the model, i.e., DistilBERT, or use the `AutoTokenizer` class that defaults to the optimized tokenizer of the models. The alternative would be to use `DistilBertTokenizerFast` directly."
      ],
      "metadata": {
        "id": "lwZUhjEcuW10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "4I30_An9uZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now input a sample sentence to the tokenizer to create a list of vocabularies with related ids and see how to access the generated ids the tokenizer generates.\n",
        "\n",
        "**❓** Why did the tokenizer generate more IDs than there are words in the input sentence?"
      ],
      "metadata": {
        "id": "TPrfty_rvDn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"Hugging Face is great!\"\n",
        "tokenized_inputs = tokenizer(input_str)\n",
        "\n",
        "# Two ways to access:\n",
        "print(tokenized_inputs.input_ids)\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "ysAbiGnAMt0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is a process of several steps. First the string is tokenized, which is slightly different from the tokenization of Tutorial 2, and then the tokens converted to IDs as shown below.\n",
        "\n",
        "The tokens shown represent so-called wordpieces, which is the result of a subword tokenization algorithm underlying the AutoTokenizer algorithms. The algorithm is called WordPiece and was developed by Google when pretraining BERT. The characters `##` are a WordPiece prefix to indicate wordpieces inside a word. See the following tutorial for [more information on WordPiece](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)."
      ],
      "metadata": {
        "id": "3J-XD6yNN_mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = tokenizer.tokenize(input_str)\n",
        "print(input_tokens)\n",
        "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "qHDezw-aOgiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert existing IDs back to text, we can use the function `decode`.\n",
        "\n"
      ],
      "metadata": {
        "id": "qqLgJD9Ype6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tokenizer.decode(input_ids)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "YZ_PXvxJpi1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Decode the original tokenized sequence `tokenized_inputs` in the code cell below. Remember that the `decode` function only takes IDs as input and not a dictionary."
      ],
      "metadata": {
        "id": "KGZnC7B9qm7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the original variable tokenized_inputs\n",
        "print(tokenized_inputs)"
      ],
      "metadata": {
        "id": "VcnegkXyqor0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the mystery of more IDs than visible words in the input sequence can partialls be answered by the tokenization and by the fact that special tokens [CLS] at the beginning of the sequence and [SEP] at the end of the sequence are added.\n",
        "\n",
        "Another way to reach the same result as with `decode` can be seen in the next code cell."
      ],
      "metadata": {
        "id": "by0OrtRepqho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer._tokenizer.encode(input_str)\n",
        "special = inputs.tokens\n",
        "print(special)"
      ],
      "metadata": {
        "id": "SUznB96BVmTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the tokenizer is a format specific to the Transformers libraries.  "
      ],
      "metadata": {
        "id": "DUBAZ4zptt-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenized_inputs))\n",
        "print(tokenized_inputs)"
      ],
      "metadata": {
        "id": "Vs3XPFGFtmAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the output of the tokenizer to a PyTorch tensor can be done easily by adding `return_tensor = pt`to the tokenization process. Compare the following output with the output of the previous code cell."
      ],
      "metadata": {
        "id": "tWaFF3eLt0PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Hugging Face is great!\", return_tensors=\"pt\")\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "YupZO-tbt-DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also tokenize a number of sentences at once. To ease processing, it is common to make all sentences of the same length by padding, that is, adding the `PAD' token to sequences to turn them all into sequences of the same length."
      ],
      "metadata": {
        "id": "OtpTYQpTuddt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer([\"Hugging Face is great!\",\n",
        "                         \"The quick brown fox jumps over the lazy dog.\",\n",
        "                         \"We are learning to fine-tune models.\",\n",
        "                         ],\n",
        "                         return_tensors=\"pt\",\n",
        "                         padding=True,\n",
        "                         truncation=True)\n",
        "print(model_inputs)\n",
        "print(tokenizer.pad_token, tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "aznHyJvaudlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to decode a whole set of sentences, we can use the function batch decode."
      ],
      "metadata": {
        "id": "0G70AWabvQ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
        "print()\n",
        "print(\"Omitting special characters when decoding:\")\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "65a-vRCUvQ_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Models from Hugging Face can be initialized very similar to Tokenizers. There are model-specific classes or the AutoModel classes, which is preferable when comparing different models.\n",
        "\n",
        "Hugging Face automatically setsup the architecture you need for a specific task when you specify the model class, e.g. `AutoModelForSequenceClassification` respectively the model-specific `DistilBertForSequenceClassification` need to be used when you want to do sentiment analysis, question-answering, etc. For training a model on the masked language task, you need to use other classes, such as `DistilBertForMaskedLM`. More details can be found [here](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt).\n",
        "\n",
        "\n",
        "The main three types of models are:\n",
        "\n",
        "\n",
        "*   Encoders, e.g. BERT\n",
        "*   Decoders, e.g. GPT-2\n",
        "*   Encoder-Decoders, e.g. BART or T5, which are Machine Translation (MT) models\n",
        "\n"
      ],
      "metadata": {
        "id": "DHl1IZ6Xv_mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)"
      ],
      "metadata": {
        "id": "SrlZsU1DxcM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get this warning because the sequence classification parameters of the model have not yet been trained.\n",
        "\n",
        "To input a string into a model requires it to be tokenized first. Then the input is very easy. The output consists of two classes since we indicated `num_labels = 2`."
      ],
      "metadata": {
        "id": "OJDH8ZR4x2f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "print(model_inputs)\n",
        "print(model_outputs)\n",
        "\n",
        "print(\"To convert the logits outputs to probabilities, we can use softmax again:\", torch.softmax(model_outputs.logits, dim=1))"
      ],
      "metadata": {
        "id": "dfwklamOx7o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have two output classes for a binary classification task, which is only due to the fact how Hugging Face calculates the loss.\n",
        "\n",
        "We can now use the model logits to calculate the loss, i.e., go from the model predictions to the intended label and calculate the difference with any suitable loss function. We will first do this using PyTorch.\n",
        "\n",
        "For this example, we pretend that Label 1 is the correct label and pass this information to a loss function. Since this is a binary classification task, it makes more sense to use sigmoid or cross-entropy than softmax as a loss function."
      ],
      "metadata": {
        "id": "14VqXP-3zHIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.tensor([1])\n",
        "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
        "print(loss)\n",
        "loss.backward()\n",
        "\n",
        "# You can get the parameters\n",
        "list(model.named_parameters())[0]"
      ],
      "metadata": {
        "id": "TpRVsTqEzF8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also calculate the loss directly with Hugging Face."
      ],
      "metadata": {
        "id": "d1_AKNztz5CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "labels = ['NEGATIVE', 'POSITIVE']\n",
        "model_inputs['labels'] = torch.tensor([1])\n",
        "\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(model_outputs)\n",
        "print()\n",
        "print(\"Model predictions: \", labels[model_outputs.logits.argmax()])"
      ],
      "metadata": {
        "id": "czgLomWB0CGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To access the attention weights and hidden states, i.e., the"
      ],
      "metadata": {
        "id": "79i-d7Lu0eoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze in more detail what happens during training, we can visualize the attention weights and hidden states.\n",
        "\n",
        "We can use BertViz to explicitly see the change of the attention weights for each layer."
      ],
      "metadata": {
        "id": "S_X87Uks2Jes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import model_view, head_view\n",
        "\n",
        "# We need to initialize the model with setting output attentions explicitly to True\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2, output_attentions=True)\n",
        "# We then need to explicitly encode the tokens\n",
        "inputs = tokenizer.encode(input_str, return_tensors='pt')\n",
        "outputs = model(inputs)\n",
        "\n",
        "attention = outputs[-1]\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
        "head_view(attention, tokens)"
      ],
      "metadata": {
        "id": "d_FnE4uN1X-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_view(attention, tokens)"
      ],
      "metadata": {
        "id": "a_ZkECvf4xMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also another method to analyze hidden states and attention weights."
      ],
      "metadata": {
        "id": "lGVmD5aC2dgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    model_output = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(\"Hidden state size (per layer):  \", model_output.hidden_states[0].shape)\n",
        "print(\"Attention head size (per layer):\", model_output.attentions[0].shape)     # (layer, batch, query_word_idx, key_word_idxs)\n",
        "                                                                               # y-axis is query, x-axis is key\n",
        "print(model_output)"
      ],
      "metadata": {
        "id": "0rv995SK2nHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
        "print(tokens)\n",
        "\n",
        "n_layers = len(model_output.attentions)\n",
        "n_heads = len(model_output.attentions[0][0])\n",
        "fig, axes = plt.subplots(6, 12)\n",
        "fig.set_size_inches(18.5*2, 10.5*2)\n",
        "for layer in range(n_layers):\n",
        "    for i in range(n_heads):\n",
        "        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n",
        "        axes[layer][i].set_xticks(list(range(8))) # 8 is the number of wordpieces in our example\n",
        "        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n",
        "        axes[layer][i].set_yticks(list(range(8))) # 8 is the number of wordpieces in our example\n",
        "        axes[layer][i].set_yticklabels(labels=tokens)\n",
        "\n",
        "        if layer == 5:\n",
        "            axes[layer, i].set(xlabel=f\"head={i}\")\n",
        "        if i == 0:\n",
        "            axes[layer, i].set(ylabel=f\"layer={layer}\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jr4zM6px2qfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "For your final projects, you will need to finetune a pretrained language model.\n",
        "\n",
        "In addition to models, Hugging Face also provides a large repository of datasets."
      ],
      "metadata": {
        "id": "hPCP2t1A3-xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the data**\n",
        "\n",
        "For this example we are going to work with the `imdb` dataset, which is a Large Movie Review Dataset.\n",
        "\n",
        "We will use the native PyTorch version to load the dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NkHrk0dn57r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "# Just take the first 50 tokens for speed on cpu\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()[:50]),\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "# Take 128 random examples for train and 32 validation\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=24).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=24).select(range(128, 160)).map(truncate),\n",
        ")"
      ],
      "metadata": {
        "id": "1xIMqRUF4STK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_imdb_dataset"
      ],
      "metadata": {
        "id": "5Tu3L15k67EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_imdb_dataset['train'][:10]"
      ],
      "metadata": {
        "id": "62bhqrp85SfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to prepare the dataset as input to the model by tokenization and padding. We also need to:\n",
        "\n",
        "1. Remove the `text` column because the model does not accept raw text as an input:\n",
        "\n",
        "    ```py\n",
        "    >>> tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "    ```\n",
        "\n",
        "2. Rename the `label` column to `labels` because the model expects the argument to be named `labels`:\n",
        "\n",
        "    ```py\n",
        "    >>> tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "    ```\n",
        "\n",
        "3. Set the format of the dataset to return PyTorch tensors instead of lists:\n",
        "\n",
        "    ```py\n",
        "    >>> tokenized_datasets.set_format(\"torch\")\n",
        "    ```"
      ],
      "metadata": {
        "id": "1mMv59IY7PRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_dataset = small_imdb_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
        "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
        "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "small_tokenized_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "4v2E6JSH7TVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now check what the first two sequences of the tokenized training dataset looks like."
      ],
      "metadata": {
        "id": "o2CZOypm7vgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_tokenized_dataset['train'][0:2]"
      ],
      "metadata": {
        "id": "kfypRDaJ7vmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a `DataLoader` for your training and test datasets so we can iterate over batches of data:"
      ],
      "metadata": {
        "id": "ilGgIfsv8ewL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\n",
        "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)"
      ],
      "metadata": {
        "id": "LdbHUcna8coc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**\n",
        "\n",
        "Hugging Face models also use `torch.nn.Module` like you did in Home Exercise 3, which means backpropagation happens the same way and the same optimizers can be used. Hugging Face includes optimizers and learning rate schedules, i.e., changes along the training process, to train Transformer models.\n",
        "\n",
        "With Stochastic Gradient Descent the learning rate does not change during training. Adam represents an optimizer that extends SGD by providing an Adaptive Gradient Algorithm (AdaGrad) and an adaptation depending on the recent magnitude of gradients called Root Mean Square Propagation (RMSProp). For more details on different optimizers, see for instance [this information](https://www.ruder.io/optimizing-gradient-descent/#adam).\n",
        "\n",
        "In this example we are going to use AdamW Optimizer, an extension of Adam with weight decay. And we're using a linear learning rate scheduler, which reduces the learning rate a little bit after each training step over the course of training.\n",
        "\n",
        "Let's load our model, optimizer, and learning rate scheduler. For this we need to the basic hyperparameters of the number of epochs we wish to train for and the number of training steps that depends on the size of the training dataset. We again get the same warning as before about this model not having been trained on sequence classification."
      ],
      "metadata": {
        "id": "97g8NNEq8pxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=5)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = 3 * len(train_dataloader)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "unJR1eVh-gNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last you can specify to use a `GPU`if you have access to one, e.g. on Colab, or else use a `CPU` if there is no access. Today, we will only train with `CPU`so no need to run the following cell."
      ],
      "metadata": {
        "id": "BgjoAGsQ_lI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "#model.to(device)"
      ],
      "metadata": {
        "id": "Oz0ioe-T_kxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop in PyTorch\n",
        "\n",
        "First, we will use native PyTorch for the training loop and training, since it is more transparent than the built-in Hugging Face functions.\n",
        "\n",
        "To keep track of your training progress, use the [tqdm](https://tqdm.github.io/) library to add a progress bar over the number of training steps.\n",
        "\n",
        "We we also include a validation step that test the current state of the model on the validation dataset and saves a version of the model called `checkpoint`. To save the different model states as checkpoints, we first create a folder called checkpoints.  \n",
        "\n",
        "You might also want to consider early stopping, that is, interrupting the training loop when a certain threshold (value) has been reached. More information on that can be found [here](https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback)."
      ],
      "metadata": {
        "id": "B4tdTuDs_3oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "njZhehhSCrdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    for batch_i, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch)\n",
        "        loss += output.loss\n",
        "\n",
        "    avg_val_loss = loss / len(eval_dataloader)\n",
        "    print(f\"Validation loss: {avg_val_loss}\")\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(\"Saving checkpoint!\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        model.save_pretrained(f\"checkpoints/epoch_{epoch}.pt\")"
      ],
      "metadata": {
        "id": "TvGKMoKFAApB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop with Hugging Face Trainer\n",
        "\n",
        "A [Hugging Face tutorial](https://huggingface.co/docs/transformers/training) on both variants of training is available. Here we will use this `Trainer` class that covers most needs.\n",
        "\n",
        "We need to load our dataset and tokenize again, since the dataset above was specified for torch."
      ],
      "metadata": {
        "id": "Zgns-iz8IwSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "#imdb_dataset = load_dataset(\"imdb\") # we had loaded the imdb dataset already above - if not, outcomment this line\n",
        "\n",
        "# Just take the first 50 tokens for speed on cpu\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()[:50]),\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "# Take 128 random examples for train and 32 validation\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=24).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=24).select(range(128, 160)).map(truncate),\n",
        ")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_dataset = small_imdb_dataset.map(tokenize_function, batched=True, batch_size=16)"
      ],
      "metadata": {
        "id": "psOqoDYvJBME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can specify all the training hyperparameters by using the `TrainningArguments`class, which is detailed [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments).\n",
        "\n",
        "The `Trainer` then performs the training und you can pass all the arguments to this class, even model checkpoints to resume training later. What was the validation part in the loop above is now the `compute_metrics` function."
      ],
      "metadata": {
        "id": "Y5cD7HihKNRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"sample_cl_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    load_best_model_at_end=True,\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return {\"accuracy\": np.mean(predictions == labels)}\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=arguments,\n",
        "    train_dataset=small_tokenized_dataset['train'],\n",
        "    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "5c0918538de84619a286443876e30f30",
            "b31bc58ffec846ad8976e29f89fc8aec",
            "39a69a18d8bb405fa488dc55bd831d12",
            "a93f320ae32243458daefc3716fd268b",
            "69310e2d8e93476ea2272fcf26fde000",
            "caca1eac3fcc4278b1804c40d0a2db97",
            "c54720d3544047e696bc90c922fca546",
            "45a470fb9e9442ff81d22d388fd09fab",
            "c880f0691fd440aa95fb729cbc11d96e",
            "6dda293ea7254ecea1847b48cceda679",
            "eabcd881271c4f298713f863bcf6a3b4",
            "777bc122c43f4ddf81089792f455ae8c",
            "fe2b8a2d9261465f8bf14afd60171f7a",
            "2154ba7754094d4c80004c04be847f80",
            "f045588006d94b57bb278b8dacaa12f7",
            "a0497345e21c409f84c070c16da62a22",
            "6ee22471aa35446e8dbcc107316c8600",
            "2f5ce43840934c8cb8a6b69534ae50fd",
            "d904479beefa440db6aa8beb8cf30256",
            "4e982db3b7164f0a8cdf9af2c7c1dd60",
            "98bdcb6ef1cd4b2abd9117a85b016729",
            "271d2901952841f1a46f2509f5e6160a"
          ]
        },
        "id": "mDOF6X5LKNYa",
        "outputId": "e31e5700-d7cd-427c-de66-8f6715583d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c0918538de84619a286443876e30f30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "777bc122c43f4ddf81089792f455ae8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "OU0IMQaINtC4",
        "outputId": "a6dc8461-349e-4fc6-ee07-e3ac26bf7227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 10:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.696094</td>\n",
              "      <td>0.468750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.697149</td>\n",
              "      <td>0.468750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.696092</td>\n",
              "      <td>0.468750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=24, training_loss=0.6932957172393799, metrics={'train_runtime': 652.7299, 'train_samples_per_second': 0.588, 'train_steps_per_second': 0.037, 'total_flos': 22199995296000.0, 'train_loss': 0.6932957172393799, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training has been completed, evaluate the model, which is very easy with Hugging Face."
      ],
      "metadata": {
        "id": "2JxJn8y7EUWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.predict(small_tokenized_dataset['val'])\n",
        "print(results)"
      ],
      "metadata": {
        "id": "xRYZ6XXyOV9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we want to load the fine-tuned model for evaluation. We can load our models just like we load models from Hugging Face only that we need to pass the path to our models to the function `from_pretrained()`. We had 24 training steps (3 epochs times 8, where the 8 is the total number of training data (128) divided by the batch size (16)) so we can load a checkpoint for each of these steps.  "
      ],
      "metadata": {
        "id": "o-awP91-OcXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"I love movies!\"\n",
        "\n",
        "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer/checkpoint-24\")\n",
        "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
        "prediction = torch.argmax(fine_tuned_model(**model_inputs).logits)\n",
        "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
      ],
      "metadata": {
        "id": "DQ3QfFHjASxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}